# ü§ñ Sentience SDK Playground

**Welcome!** This playground contains interactive demos comparing different approaches to LLM-powered web automation. These demos show the **dramatic difference** between using structured semantic data vs. vision-only approaches for browser automation.

> **TL;DR**: SDK + semantic geometry achieves **100% success** with **73% fewer tokens**, while vision-only approaches fail completely. See the comparison reports for details!

---

## üéØ What's Inside

### üõí [Amazon Shopping Demo](amazon_shopping/)
**Task**: Search for "Christmas gift", select a product, add to cart

**Results**:
- **SDK Approach**: ‚úÖ 100% success (optimized: 19,956 tokens)
- **Vision Approach**: ‚ùå 0% success (0/3 runs completed)

[üìä Full Comparison Report](docs/DEMO_COMPARISON_REPORT.md)

### üîç [Google Search Demo](google_search/)
**Task**: Search for "visiting japan", click a non-ad result

**Results**:
- **SDK Approach**: ‚úÖ 100% success (2,636 tokens with 73% optimization)
- **Vision Approach**: ‚ùå 0% success (0/2 runs completed)

[üìä Full Comparison Report](google_search/GOOGLE_SEARCH_COMPARISON_REPORT.md)

---

## üöÄ Quick Start

### 1. Clone & Install

```bash
# Clone the repo
git clone https://github.com/SentienceAPI/sentience-sdk-playground.git
cd sdk-python/playground

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -e ..  # Install Sentience SDK

# Install Playwright browsers
playwright install chromium
```

### 2. Set Up API Keys

```bash
# Copy the example env file
cp .env.example .env

# Edit .env and add your API keys
# You'll need:
# - OPENAI_API_KEY (required for LLM)
# - SENTIENCE_API_KEY (optional, for API-based filtering)
```

### 3. Run a Demo!

```bash
# Google Search (simplest, fastest)
cd google_search
./run_demo1.sh  # SDK approach
./run_demo2.sh  # Vision approach (spoiler: it fails)

# Amazon Shopping (more complex)
cd amazon_shopping
./run_demo1.sh  # SDK approach
./run_demo2.sh  # Vision approach (also fails)
```

---

## üìÅ Demo Structure

### Demo 1: SDK + Semantic Geometry (‚úÖ Works!)

**How it works**:
1. Navigate to webpage
2. Call `snapshot()` to get structured JSON with:
   - Element roles (button, link, textbox, etc.)
   - Bounding boxes (x, y, width, height)
   - Visual cues (is_primary, is_clickable, etc.)
   - Viewport visibility, z-index, occlusion
3. **Filter** elements by role (exclude decorative SVG, layout divs)
4. Send only relevant elements to LLM
5. LLM returns element ID + action
6. Execute action with precise `(x, y)` coordinates

**Why it wins**:
- ‚úÖ Deterministic (same input ‚Üí same output)
- ‚úÖ Optimizable (73% token reduction through filtering)
- ‚úÖ Reliable (100% success rate)
- ‚úÖ Economical (only pay for successes)

### Demo 2: Vision + GPT-4o (‚ùå Fails!)

**How it works**:
1. Navigate to webpage
2. Take full-page screenshot (1920√ó1080 pixels)
3. Send entire image to GPT-4o Vision
4. LLM tries to identify elements visually
5. Returns estimated coordinates
6. Attempt to click coordinates

**Why it fails**:
- ‚ùå No semantic understanding (can't distinguish link from button)
- ‚ùå Content policy triggers (automation flagged as suspicious)
- ‚ùå Coordinate guessing (visual estimation unreliable)
- ‚ùå Not optimizable (can't filter pixels meaningfully)
- ‚ùå Wastes tokens on failures (0% success rate)

---

## üé¨ Demo Videos

### Google Search Demo (SDK + GPT-4 Turbo)
<video src="google_search/demo1_sdk/video/demo1_google_search_20251222_183516.mp4" controls width="100%"></video>

**Features**: Search for "visiting japan" and click first non-ad result (2,636 tokens, 100% success)

### Amazon Shopping Demo (SDK + GPT-4 Turbo)
<video src="amazon_shopping/demo1_sdk_llm/video/demo1_sdk_final.mp4" controls width="100%"></video>

**Features**: Search for "Christmas gift", select product, add to cart (19,956 tokens, 100% success)

### Video Features
Each demo video includes:
- **Ken Burns effect**: Smooth zoom + pan on static screenshots
- **Token overlays**: Real-time token usage displayed on screen
- **Bounding boxes**: Visual debugging of API-filtered elements (SDK only)
- **Scene transitions**: Clear progression through automation steps

---

## üìä Key Results

### Token Optimization

| Demo | Before Optimization | After Optimization | Savings |
|------|---------------------|-------------------|---------|
| Google Search | 9,800 tokens | 2,636 tokens | **73%** |
| Amazon Shopping | ~35,000 tokens | 19,956 tokens | **43%** |

**How?** Smart element filtering:
- Scene 1: Exclude `["img", "svg", "path", "button", "link"]` ‚Üí Keep only search inputs
- Scene 3: Exclude `["div", "span", "ul", "li"]` ‚Üí Keep only result links
- Ad filtering: Remove elements containing "Ad", "Sponsored"

### Success Rates

| Approach | Amazon (5 scenes) | Google (4 scenes) |
|----------|------------------|-------------------|
| **SDK + Semantic** | ‚úÖ 100% (multiple runs) | ‚úÖ 100% (2/2) |
| **Vision Only** | ‚ùå 0% (0/3 runs) | ‚ùå 0% (0/2 runs) |

### Actual Demo Observations

**üîç Google Search Demo (SDK)**:
- Scene 1: API returned 49 elements ‚Üí Filtered to 1 combobox ‚Üí LLM found search box instantly
- Scene 3: API returned 50 elements ‚Üí Filtered to 7-8 links ‚Üí LLM selected first non-ad result
- Navigation: URL changed successfully, landed on Japan tourism page
- **Total**: 2,636 tokens, 100% success

**üîç Google Search Demo (Vision)**:
- Attempted 2 runs
- **Run 1**: Vision LLM returned empty response when trying to find search results
- **Run 2**: Vision LLM refused again with empty response
- Could not identify search box or distinguish ads from organic results
- **Total**: 0% success, wasted tokens on failures

**üõí Amazon Shopping Demo (SDK)**:
- Successfully navigated all 5 scenes: Homepage ‚Üí Search ‚Üí Results ‚Üí Product ‚Üí Cart
- Element filtering reduced token usage by 43%
- LLM correctly identified: search bar, product links, Add to Cart button, success message
- **Total**: 19,956 tokens (optimized), 100% success

**üõí Amazon Shopping Demo (Vision)**:
- Attempted 3 runs, all failed
- **Common failures**:
  - Never typed "Christmas gift" into search bar
  - Somehow navigated to vehicle parts category (wrong page)
  - Could not identify products or Add to Cart button
  - Vision LLM analyzing wrong pages, making random clicks
- **Total**: 0/3 successful completions

**üí° Key Observation**: The Vision approach doesn't just fail occasionally - it fails *systematically* because it lacks semantic understanding. It can't reliably distinguish a button from a link, or an ad from a search result, just by looking at pixels.

---

## üõ†Ô∏è What You Can Learn

### 1. **Element Filtering Optimization**
See [google_search/demo1_sdk/main.py](google_search/demo1_sdk/main.py) for aggressive role-based filtering:

```python
# Scene 1: Find search box
filtered_data = filter_elements(snapshot_data, exclude_roles=[
    "img", "image", "button", "link", "span", "div",
    "svg", "path", "g", "rect", "circle"
])
# Result: 49 elements ‚Üí 1 element (the search input!)
```

### 2. **LLM Prompt Engineering**
Both demos use structured prompts with:
- Clear task description
- Element selection criteria
- JSON response format
- Fallback handling

### 3. **Error Handling & Validation**
- Coordinate validation before clicking
- Element existence checks
- Navigation verification (URL changes)
- Retry logic with fallbacks

### 4. **Video Generation**
MoviePy-based video creation with:
- PIL-based text overlays (no ImageMagick needed)
- Ken Burns effect for dynamic motion
- Token usage tracking
- Scene concatenation

---

## üîß Customization

### Create Your Own Demo

```python
from sentience import SentienceBrowser, snapshot, click_rect

with SentienceBrowser(headless=False) as browser:
    # Navigate
    browser.page.goto("https://example.com")

    # Get structured data
    snap = snapshot(browser, use_api=True)

    # Filter elements (customize as needed)
    relevant = [e for e in snap.elements if e.role == "button"]

    # Ask LLM to choose
    result = llm.analyze(relevant, "Click the submit button")

    # Execute action
    click_rect(browser, result['bbox'])
```

### Modify Element Filtering

Edit `filter_elements()` in each demo's `main.py` to customize:
- Which roles to exclude
- Text-based filtering (e.g., ads, headers)
- Viewport visibility requirements
- Z-index prioritization

---

## üìö Documentation

- [Google Search Demo Plan](google_search/GOOGLE_SEARCH_DEMO_PLAN.md)
- [Amazon Shopping Demo Plan](amazon_shopping/AMAZON_SHOPPING_GUIDE.md)
- [Google Comparison Report](google_search/GOOGLE_SEARCH_COMPARISON_REPORT.md)
- [Amazon Comparison Report](amazon_shopping/Amazon_Shopping_DEMO_COMPARISON_REPORT.md)
- [Setup Guide](docs/SETUP.md)
- [Quickstart Guide](docs/QUICKSTART.md)

---

## ‚öôÔ∏è SDK
- [Sentience Python SDK](https://github.com/SentienceAPI/sentience-python)
- [Sentience TypeScript SDK](https://github.com/SentienceAPI/sentience-ts)

### "Vision LLM returned empty response"
- Vision models often refuse automation tasks (content policy)
- Reframe prompts as "UI testing" instead of "automation"
- Even then, expect low success rates

### "No module named 'moviepy.editor'"
- Use MoviePy 1.x, not 2.x: `pip install moviepy==1.0.3`
- Also need Pillow <10.0: `pip install "Pillow<10.0"`

### "Chromium crashes on macOS"
- Upgrade Playwright: `pip install playwright>=1.57.0`
- Reinstall browsers: `playwright install chromium`

---

## üéì Key Takeaways

### Why Sentience SDK Wins

1. **Semantic Understanding**: Knows a button is a button, not just pixels
2. **Deterministic Behavior**: Same elements ‚Üí same results every time
3. **Intelligent Filtering**: Can exclude 73% of noise without losing accuracy
4. **Production Ready**: 100% success rate across multiple test scenarios
5. **Cost Effective**: Only pay tokens when tasks succeed, no vision model tax

### Why Vision Fails

1. **No Semantic Info**: Can't distinguish roles from appearance alone
2. **Content Policies**: LLMs refuse automation-like requests
3. **Coordinate Guessing**: Visual estimation is inherently unreliable
4. **Not Optimizable**: Can't "filter out unimportant pixels"
5. **Economic Disaster**: Pay full token cost for 0% success rate

---

## ü§ù Contributing

We welcome contributions! Ideas for new demos:
- Form filling automation
- Multi-step checkout flows
- Social media interactions
- Document navigation and extraction
- Dynamic content handling

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](../LICENSE) file for details.

---

## üôè Credits

Built with:
- [Sentience SDK Python](https://github.com/SentienceAPI/sentience-python) - Semantic web automation
- [OpenAI GPT-4](https://platform.openai.com/) - Language model
- [Playwright](https://playwright.dev/) - Browser automation
- [MoviePy](https://zulko.github.io/moviepy/) - Video generation

---

## üöÄ Get Started Now!

```bash
cd google_search
./run_demo1.sh
```

Watch the SDK approach **flawlessly** execute a Google search in under a minute, using just 2,636 tokens.

Then try `./run_demo2.sh` to watch the vision model

**Happy automating!** üéâ
**Author:** Claude \Anthropic